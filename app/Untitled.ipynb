{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import string\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "#from textblob.wordnet import VERB\n",
    "#import nltk\n",
    "#nltk.download()\n",
    "# Download window opens, fetch wordnet\n",
    "#from nltk.corpus import wordnet as wn\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "The titular threat of The Blob has always struck me as the ultimate movie\n",
    "monster: an insatiably hungry, amoeba-like mass able to penetrate\n",
    "virtually any safeguard, capable of--as a doomed doctor chillingly\n",
    "describes it--\"assimilating flesh on contact.\n",
    "Snide comparisons to gelatin be damned, it's a concept with the most\n",
    "devastating of potential consequences, not unlike the grey goo scenario\n",
    "proposed by technological theorists fearful of\n",
    "artificial intelligence run rampant.\n",
    "'''\n",
    "\n",
    "blob = TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TextBlob' object has no attribute 'n_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-9f919b3b203e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mblob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'TextBlob' object has no attribute 'n_words'"
     ]
    }
   ],
   "source": [
    "blob.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['The', 'titular', 'threat', 'of', 'The', 'Blob', 'has', 'always', 'struck', 'me', 'as', 'the', 'ultimate', 'movie', 'monster:', 'an', 'insatiably', 'hungry,', 'amoeba-like', 'mass', 'able', 'to', 'penetrate', 'virtually', 'any', 'safeguard,', 'capable', 'of--as', 'a', 'doomed', 'doctor', 'chillingly', 'describes', 'it--\"assimilating', 'flesh', 'on', 'contact.', 'Snide', 'comparisons', 'to', 'gelatin', 'be', 'damned,', \"it's\", 'a', 'concept', 'with', 'the', 'most', 'devastating', 'of', 'potential', 'consequences,', 'not', 'unlike', 'the', 'grey', 'goo', 'scenario', 'proposed', 'by', 'technological', 'theorists', 'fearful', 'of', 'artificial', 'intelligence', 'run', 'rampant.'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['\\n'],\n",
       " ['T'],\n",
       " ['h'],\n",
       " ['e'],\n",
       " [' '],\n",
       " ['t'],\n",
       " ['i'],\n",
       " ['t'],\n",
       " ['u'],\n",
       " ['l'],\n",
       " ['a'],\n",
       " ['r'],\n",
       " [' '],\n",
       " ['t'],\n",
       " ['h'],\n",
       " ['r'],\n",
       " ['e'],\n",
       " ['a'],\n",
       " ['t'],\n",
       " [' '],\n",
       " ['o'],\n",
       " ['f'],\n",
       " [' '],\n",
       " ['T'],\n",
       " ['h'],\n",
       " ['e'],\n",
       " [' '],\n",
       " ['B'],\n",
       " ['l'],\n",
       " ['o'],\n",
       " ['b'],\n",
       " [' '],\n",
       " ['h'],\n",
       " ['a'],\n",
       " ['s'],\n",
       " [' '],\n",
       " ['a'],\n",
       " ['l'],\n",
       " ['w'],\n",
       " ['a'],\n",
       " ['y'],\n",
       " ['s'],\n",
       " [' '],\n",
       " ['s'],\n",
       " ['t'],\n",
       " ['r'],\n",
       " ['u'],\n",
       " ['c'],\n",
       " ['k'],\n",
       " [' '],\n",
       " ['m'],\n",
       " ['e'],\n",
       " [' '],\n",
       " ['a'],\n",
       " ['s'],\n",
       " [' '],\n",
       " ['t'],\n",
       " ['h'],\n",
       " ['e'],\n",
       " [' '],\n",
       " ['u'],\n",
       " ['l'],\n",
       " ['t'],\n",
       " ['i'],\n",
       " ['m'],\n",
       " ['a'],\n",
       " ['t'],\n",
       " ['e'],\n",
       " [' '],\n",
       " ['m'],\n",
       " ['o'],\n",
       " ['v'],\n",
       " ['i'],\n",
       " ['e'],\n",
       " ['\\n'],\n",
       " ['m'],\n",
       " ['o'],\n",
       " ['n'],\n",
       " ['s'],\n",
       " ['t'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " [':'],\n",
       " [' '],\n",
       " ['a'],\n",
       " ['n'],\n",
       " [' '],\n",
       " ['i'],\n",
       " ['n'],\n",
       " ['s'],\n",
       " ['a'],\n",
       " ['t'],\n",
       " ['i'],\n",
       " ['a'],\n",
       " ['b'],\n",
       " ['l'],\n",
       " ['y'],\n",
       " [' '],\n",
       " ['h'],\n",
       " ['u'],\n",
       " ['n'],\n",
       " ['g'],\n",
       " ['r'],\n",
       " ['y'],\n",
       " [','],\n",
       " [' '],\n",
       " ['a'],\n",
       " ['m'],\n",
       " ['o'],\n",
       " ['e'],\n",
       " ['b'],\n",
       " ['a'],\n",
       " ['-'],\n",
       " ['l'],\n",
       " ['i'],\n",
       " ['k'],\n",
       " ['e'],\n",
       " [' '],\n",
       " ['m'],\n",
       " ['a'],\n",
       " ['s'],\n",
       " ['s'],\n",
       " [' '],\n",
       " ['a'],\n",
       " ['b'],\n",
       " ['l'],\n",
       " ['e'],\n",
       " [' '],\n",
       " ['t'],\n",
       " ['o'],\n",
       " [' '],\n",
       " ['p'],\n",
       " ['e'],\n",
       " ['n'],\n",
       " ['e'],\n",
       " ['t'],\n",
       " ['r'],\n",
       " ['a'],\n",
       " ['t'],\n",
       " ['e'],\n",
       " ['\\n'],\n",
       " ['v'],\n",
       " ['i'],\n",
       " ['r'],\n",
       " ['t'],\n",
       " ['u'],\n",
       " ['a'],\n",
       " ['l'],\n",
       " ['l'],\n",
       " ['y'],\n",
       " [' '],\n",
       " ['a'],\n",
       " ['n'],\n",
       " ['y'],\n",
       " [' '],\n",
       " ['s'],\n",
       " ['a'],\n",
       " ['f'],\n",
       " ['e'],\n",
       " ['g'],\n",
       " ['u'],\n",
       " ['a'],\n",
       " ['r'],\n",
       " ['d'],\n",
       " [','],\n",
       " [' '],\n",
       " ['c'],\n",
       " ['a'],\n",
       " ['p'],\n",
       " ['a'],\n",
       " ['b'],\n",
       " ['l'],\n",
       " ['e'],\n",
       " [' '],\n",
       " ['o'],\n",
       " ['f'],\n",
       " ['-'],\n",
       " ['-'],\n",
       " ['a'],\n",
       " ['s'],\n",
       " [' '],\n",
       " ['a'],\n",
       " [' '],\n",
       " ['d'],\n",
       " ['o'],\n",
       " ['o'],\n",
       " ['m'],\n",
       " ['e'],\n",
       " ['d'],\n",
       " [' '],\n",
       " ['d'],\n",
       " ['o'],\n",
       " ['c'],\n",
       " ['t'],\n",
       " ['o'],\n",
       " ['r'],\n",
       " [' '],\n",
       " ['c'],\n",
       " ['h'],\n",
       " ['i'],\n",
       " ['l'],\n",
       " ['l'],\n",
       " ['i'],\n",
       " ['n'],\n",
       " ['g'],\n",
       " ['l'],\n",
       " ['y'],\n",
       " ['\\n'],\n",
       " ['d'],\n",
       " ['e'],\n",
       " ['s'],\n",
       " ['c'],\n",
       " ['r'],\n",
       " ['i'],\n",
       " ['b'],\n",
       " ['e'],\n",
       " ['s'],\n",
       " [' '],\n",
       " ['i'],\n",
       " ['t'],\n",
       " ['-'],\n",
       " ['-'],\n",
       " ['\"'],\n",
       " ['a'],\n",
       " ['s'],\n",
       " ['s'],\n",
       " ['i'],\n",
       " ['m'],\n",
       " ['i'],\n",
       " ['l'],\n",
       " ['a'],\n",
       " ['t'],\n",
       " ['i'],\n",
       " ['n'],\n",
       " ['g'],\n",
       " [' '],\n",
       " ['f'],\n",
       " ['l'],\n",
       " ['e'],\n",
       " ['s'],\n",
       " ['h'],\n",
       " [' '],\n",
       " ['o'],\n",
       " ['n'],\n",
       " [' '],\n",
       " ['c'],\n",
       " ['o'],\n",
       " ['n'],\n",
       " ['t'],\n",
       " ['a'],\n",
       " ['c'],\n",
       " ['t'],\n",
       " ['.'],\n",
       " ['\\n'],\n",
       " ['S'],\n",
       " ['n'],\n",
       " ['i'],\n",
       " ['d'],\n",
       " ['e'],\n",
       " [' '],\n",
       " ['c'],\n",
       " ['o'],\n",
       " ['m'],\n",
       " ['p'],\n",
       " ['a'],\n",
       " ['r'],\n",
       " ['i'],\n",
       " ['s'],\n",
       " ['o'],\n",
       " ['n'],\n",
       " ['s'],\n",
       " [' '],\n",
       " ['t'],\n",
       " ['o'],\n",
       " [' '],\n",
       " ['g'],\n",
       " ['e'],\n",
       " ['l'],\n",
       " ['a'],\n",
       " ['t'],\n",
       " ['i'],\n",
       " ['n'],\n",
       " [' '],\n",
       " ['b'],\n",
       " ['e'],\n",
       " [' '],\n",
       " ['d'],\n",
       " ['a'],\n",
       " ['m'],\n",
       " ['n'],\n",
       " ['e'],\n",
       " ['d'],\n",
       " [','],\n",
       " [' '],\n",
       " ['i'],\n",
       " ['t'],\n",
       " [\"'\"],\n",
       " ['s'],\n",
       " [' '],\n",
       " ['a'],\n",
       " [' '],\n",
       " ['c'],\n",
       " ['o'],\n",
       " ['n'],\n",
       " ['c'],\n",
       " ['e'],\n",
       " ['p'],\n",
       " ['t'],\n",
       " [' '],\n",
       " ['w'],\n",
       " ['i'],\n",
       " ['t'],\n",
       " ['h'],\n",
       " [' '],\n",
       " ['t'],\n",
       " ['h'],\n",
       " ['e'],\n",
       " [' '],\n",
       " ['m'],\n",
       " ['o'],\n",
       " ['s'],\n",
       " ['t'],\n",
       " ['\\n'],\n",
       " ['d'],\n",
       " ['e'],\n",
       " ['v'],\n",
       " ['a'],\n",
       " ['s'],\n",
       " ['t'],\n",
       " ['a'],\n",
       " ['t'],\n",
       " ['i'],\n",
       " ['n'],\n",
       " ['g'],\n",
       " [' '],\n",
       " ['o'],\n",
       " ['f'],\n",
       " [' '],\n",
       " ['p'],\n",
       " ['o'],\n",
       " ['t'],\n",
       " ['e'],\n",
       " ['n'],\n",
       " ['t'],\n",
       " ['i'],\n",
       " ['a'],\n",
       " ['l'],\n",
       " [' '],\n",
       " ['c'],\n",
       " ['o'],\n",
       " ['n'],\n",
       " ['s'],\n",
       " ['e'],\n",
       " ['q'],\n",
       " ['u'],\n",
       " ['e'],\n",
       " ['n'],\n",
       " ['c'],\n",
       " ['e'],\n",
       " ['s'],\n",
       " [','],\n",
       " [' '],\n",
       " ['n'],\n",
       " ['o'],\n",
       " ['t'],\n",
       " [' '],\n",
       " ['u'],\n",
       " ['n'],\n",
       " ['l'],\n",
       " ['i'],\n",
       " ['k'],\n",
       " ['e'],\n",
       " [' '],\n",
       " ['t'],\n",
       " ['h'],\n",
       " ['e'],\n",
       " [' '],\n",
       " ['g'],\n",
       " ['r'],\n",
       " ['e'],\n",
       " ['y'],\n",
       " [' '],\n",
       " ['g'],\n",
       " ['o'],\n",
       " ['o'],\n",
       " [' '],\n",
       " ['s'],\n",
       " ['c'],\n",
       " ['e'],\n",
       " ['n'],\n",
       " ['a'],\n",
       " ['r'],\n",
       " ['i'],\n",
       " ['o'],\n",
       " ['\\n'],\n",
       " ['p'],\n",
       " ['r'],\n",
       " ['o'],\n",
       " ['p'],\n",
       " ['o'],\n",
       " ['s'],\n",
       " ['e'],\n",
       " ['d'],\n",
       " [' '],\n",
       " ['b'],\n",
       " ['y'],\n",
       " [' '],\n",
       " ['t'],\n",
       " ['e'],\n",
       " ['c'],\n",
       " ['h'],\n",
       " ['n'],\n",
       " ['o'],\n",
       " ['l'],\n",
       " ['o'],\n",
       " ['g'],\n",
       " ['i'],\n",
       " ['c'],\n",
       " ['a'],\n",
       " ['l'],\n",
       " [' '],\n",
       " ['t'],\n",
       " ['h'],\n",
       " ['e'],\n",
       " ['o'],\n",
       " ['r'],\n",
       " ['i'],\n",
       " ['s'],\n",
       " ['t'],\n",
       " ['s'],\n",
       " [' '],\n",
       " ['f'],\n",
       " ['e'],\n",
       " ['a'],\n",
       " ['r'],\n",
       " ['f'],\n",
       " ['u'],\n",
       " ['l'],\n",
       " [' '],\n",
       " ['o'],\n",
       " ['f'],\n",
       " ['\\n'],\n",
       " ['a'],\n",
       " ['r'],\n",
       " ['t'],\n",
       " ['i'],\n",
       " ['f'],\n",
       " ['i'],\n",
       " ['c'],\n",
       " ['i'],\n",
       " ['a'],\n",
       " ['l'],\n",
       " [' '],\n",
       " ['i'],\n",
       " ['n'],\n",
       " ['t'],\n",
       " ['e'],\n",
       " ['l'],\n",
       " ['l'],\n",
       " ['i'],\n",
       " ['g'],\n",
       " ['e'],\n",
       " ['n'],\n",
       " ['c'],\n",
       " ['e'],\n",
       " [' '],\n",
       " ['r'],\n",
       " ['u'],\n",
       " ['n'],\n",
       " [' '],\n",
       " ['r'],\n",
       " ['a'],\n",
       " ['m'],\n",
       " ['p'],\n",
       " ['a'],\n",
       " ['n'],\n",
       " ['t'],\n",
       " ['.'],\n",
       " ['\\n']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[split(w) for w in blob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.word_counts['intelligence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger')\n",
      "  \u001b[0m\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\Gean/nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\Gean\\\\Anaconda3\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\Gean\\\\Anaconda3\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\Gean\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n"
     ]
    },
    {
     "ename": "MissingCorpusError",
     "evalue": "\nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\decorators.py\u001b[0m in \u001b[0;36mdecorated\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\en\\taggers.py\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \"\"\"\n\u001b[1;32m--> 133\u001b[1;33m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             \u001b[0mAP_MODEL_LOC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'file:'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'taggers/averaged_perceptron_tagger/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mPICKLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\Gean/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\Gean\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Gean\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Gean\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMissingCorpusError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-30ab52a145e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mblob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\decorators.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36mpos_tags\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    476\u001b[0m         \"\"\"\n\u001b[0;32m    477\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 478\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mval\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tags\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    479\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m             return [(Word(word, pos_tag=t), unicode(t))\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    476\u001b[0m         \"\"\"\n\u001b[0;32m    477\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 478\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mval\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tags\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    479\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m             return [(Word(word, pos_tag=t), unicode(t))\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\decorators.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36mpos_tags\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m             return [(Word(word, pos_tag=t), unicode(t))\n\u001b[1;32m--> 481\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    482\u001b[0m                     if not PUNCTUATION_REGEX.match(unicode(t))]\n\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\decorators.py\u001b[0m in \u001b[0;36mdecorated\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMissingCorpusError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMissingCorpusError\u001b[0m: \nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n"
     ]
    }
   ],
   "source": [
    "blob.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The `text` argument passed to `__init__(text)` must be a string, not <class '_io.TextIOWrapper'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-0ae7c790f572>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sentenca_lula.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mblob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, text, tokenizer, pos_tagger, np_extractor, analyzer, parser, classifier, clean_html)\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m             raise TypeError('The `text` argument passed to `__init__(text)` '\n\u001b[1;32m--> 364\u001b[1;33m                             'must be a string, not {0}'.format(type(text)))\n\u001b[0m\u001b[0;32m    365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclean_html\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m             raise NotImplementedError(\"clean_html has been deprecated. \"\n",
      "\u001b[1;31mTypeError\u001b[0m: The `text` argument passed to `__init__(text)` must be a string, not <class '_io.TextIOWrapper'>"
     ]
    }
   ],
   "source": [
    "with open('sentenca_lula.txt', encoding='utf8') as infile:\n",
    "    blob = TextBlob(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 27757: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-3302fd09dfd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0munidecode\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0munidecode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sentenca_lula.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbobo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munidecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 27757: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "file=open(\"sentenca_lula.txt\");\n",
    "t=file.read();\n",
    "print(type(t))\n",
    "bobo = TextBlob(unidecode(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 27757: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-35c37568bc52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sentenca_lula.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m      \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mblob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 27757: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "with open('sentenca_lula.txt', 'r') as fp:\n",
    "     content = fp.read()\n",
    "blob = TextBlob(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sys' has no attribute 'setdefaultencoding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-d841b849826e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefaultencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sentenca_lula.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'sys' has no attribute 'setdefaultencoding'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.setdefaultencoding('utf8')\n",
    "from textblob import TextBlob\n",
    "file=open(\"sentenca_lula.txt\");\n",
    "t=file.read();\n",
    "print(type(t))\n",
    "bobo = TextBlob(unidecode(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "text = open('sentenca_lula.doc', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11176"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import docx\n",
    "doc = docx.Document('sentenca_lula.docx')\n",
    "docstr = str(doc)\n",
    "len(doc.paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('sentenca_lula.txt', encoding='utf8')\n",
    "text.read()\n",
    "\n",
    "lula = TextBlob(str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('sentenca_lula.txt', 'r', encoding='utf8')\n",
    "raw= text.read()\n",
    "text2 = TextBlob(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordList(st):\n",
    "    st = st.lower()\n",
    "    st = st.split()\n",
    "    return st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordRank(st):\n",
    "    allWords = wordList(st)\n",
    "    nonRepeatWords = []\n",
    "    dictionary = {}\n",
    "    # 1. Create a non repeat list of words\n",
    "    for word in allWords:\n",
    "        if word not in nonRepeatWords:\n",
    "            nonRepeatWords.append(word)\n",
    "    # 2. create a dictionary where every word is a key\n",
    "    # and the number of times it repeats is a value\n",
    "    for word in nonRepeatWords:\n",
    "        dictionary[word] = allWords.count(word)\n",
    "        \n",
    "    # 3. We are going to use the dictionary and the local\n",
    "    # sorted function to print out a ranked list!\n",
    "    \n",
    "    rankedList = sorted(dictionary, key=dictionary.get, reverse=True)\n",
    "    \n",
    "    for i in range(0, len(rankedList)):\n",
    "        key = rankedList[i]\n",
    "        value = dictionary[key]\n",
    "        print(key+' repeat: '+str(value) + ' times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-d3b444e99793>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwordRank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-83-221d0115feba>\u001b[0m in \u001b[0;36mwordRank\u001b[1;34m(st)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# and the number of times it repeats is a value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnonRepeatWords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mdictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mallWords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# 3. We are going to use the dictionary and the local\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self, strg, case_sensitive, *args, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \"\"\"\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcase_sensitive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             return [word.lower() for word in self].count(strg.lower(), *args,\n\u001b[0m\u001b[0;32m    251\u001b[0m                     **kwargs)\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \"\"\"\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcase_sensitive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             return [word.lower() for word in self].count(strg.lower(), *args,\n\u001b[0m\u001b[0;32m    251\u001b[0m                     **kwargs)\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wordRank(text2)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://realpython.com/flask-by-example-part-3-text-processing-with-requests-beautifulsoup-nltk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='sentenca_lula.txt' mode='r' encoding='utf8'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1590909090909091"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931818181818182"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method WordList.singularize of WordList(['The', 'titular', 'threat', 'of', 'The', 'Blob', 'has', 'always', 'struck', 'me', 'as', 'the', 'ultimate', 'movie', 'monster', 'an', 'insatiably', 'hungry', 'amoeba-like', 'mass', 'able', 'to', 'penetrate', 'virtually', 'any', 'safeguard', 'capable', 'of', 'as', 'a', 'doomed', 'doctor', 'chillingly', 'describes', 'it', 'assimilating', 'flesh', 'on', 'contact', 'Snide', 'comparisons', 'to', 'gelatin', 'be', 'damned', 'it', \"'s\", 'a', 'concept', 'with', 'the', 'most', 'devastating', 'of', 'potential', 'consequences', 'not', 'unlike', 'the', 'grey', 'goo', 'scenario', 'proposed', 'by', 'technological', 'theorists', 'fearful', 'of', 'artificial', 'intelligence', 'run', 'rampant'])>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.words.singularize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method WordList.pluralize of WordList(['The', 'titular', 'threat', 'of', 'The', 'Blob', 'has', 'always', 'struck', 'me', 'as', 'the', 'ultimate', 'movie', 'monster', 'an', 'insatiably', 'hungry', 'amoeba-like', 'mass', 'able', 'to', 'penetrate', 'virtually', 'any', 'safeguard', 'capable', 'of', 'as', 'a', 'doomed', 'doctor', 'chillingly', 'describes', 'it', 'assimilating', 'flesh', 'on', 'contact', 'Snide', 'comparisons', 'to', 'gelatin', 'be', 'damned', 'it', \"'s\", 'a', 'concept', 'with', 'the', 'most', 'devastating', 'of', 'potential', 'consequences', 'not', 'unlike', 'the', 'grey', 'goo', 'scenario', 'proposed', 'by', 'technological', 'theorists', 'fearful', 'of', 'artificial', 'intelligence', 'run', 'rampant'])>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.words.pluralize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.wordnet import Synset\n",
    ">>> octopus = Synset('octopus.n.02')\n",
    ">>> shrimp = Synset('shrimp.n.03')\n",
    ">>> octopus.path_similarity(shrimp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spelling correct\n",
    "b = TextBlob(\"I havv goood speling!\")\n",
    ">>> print(b.correct())\n",
    "I have good spelling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spelling correct\n",
    "from textblob import Word\n",
    ">>> w = Word('falibility')\n",
    ">>> w.spellcheck()\n",
    "[('fallibility', 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common words from dataframe\n",
    "Counter(\" \".join(df[\"text\"]).split()).most_common(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Translation and Language Detection¶\n",
    "en_blob = TextBlob(u'Simple is better than complex.')\n",
    "en_blob.translate(to='es')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_blob = TextBlob(u\"美丽优于丑陋\")\n",
    ">>> chinese_blob.translate(from_lang=\"zh-CN\", to='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = TextBlob(u\"بسيط هو أفضل من مجمع\")\n",
    ">>> b.detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = TextBlob(\"And now for something completely different.\")\n",
    ">>> print(b.parse())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams are used for a variety of different task. For example, when developing a language model, n-grams are used to develop not just unigram models but also bigram and trigram models. Google and Microsoft have developed web scale n-gram models that can be used in a variety of tasks such as spelling correction, word breaking and text summarization. Here is a publicly available web scale n-gram model by Microsoft: http://research.microsoft.com/en-us/collaboration/focus/cs/web-ngram.aspx. Here is a paper that uses Web N-gram models for text summarization:Micropinion Generation: An Unsupervised Approach to Generating Ultra-Concise Summaries of Opinions \n",
    "\n",
    "Another use of n-grams is for developing features for supervised Machine Learning models such as SVMs, MaxEnt models, Naive Bayes, etc. The idea is to use tokens such as bigrams in the feature space instead of just unigrams. But please be warned that from my personal experience and various research papers that I have reviewed, the use of bigrams and trigrams in your feature space may not necessarily yield any significant improvement. The only way to know this is to try it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-grams\n",
    "blob = TextBlob(\"Now is better than never.\")\n",
    ">>> blob.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    " \n",
    "print(SnowballStemmer.languages)\n",
    "'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eu vou para a copa do mund\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "french_stemmer = SnowballStemmer('portuguese')\n",
    "print(french_stemmer.stem(\"Eu vou para a Copa do Mundo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "french word\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    " \n",
    "french_stemmer = SnowballStemmer('french')\n",
    " \n",
    "print(french_stemmer.stem(\"French word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stone\n",
      "speak\n",
      "bedroom\n",
      "joke\n",
      "lisa\n",
      "purpl\n",
      "----------------------\n",
      "stone\n",
      "speaking\n",
      "bedroom\n",
      "joke\n",
      "lisa\n",
      "purple\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "from nltk.stem import PorterStemmer\n",
    " \n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "print(stemmer.stem('stones'))\n",
    " \n",
    "print(stemmer.stem('speaking'))\n",
    " \n",
    "print(stemmer.stem('bedroom'))\n",
    " \n",
    "print(stemmer.stem('jokes'))\n",
    " \n",
    "print(stemmer.stem('lisa'))\n",
    " \n",
    "print(stemmer.stem('purple'))\n",
    " \n",
    "print('----------------------')\n",
    " \n",
    "print(lemmatizer.lemmatize('stones'))\n",
    " \n",
    "print(lemmatizer.lemmatize('speaking'))\n",
    " \n",
    "print(lemmatizer.lemmatize('bedroom'))\n",
    " \n",
    "print(lemmatizer.lemmatize('jokes'))\n",
    " \n",
    "print(lemmatizer.lemmatize('lisa'))\n",
    " \n",
    "print(lemmatizer.lemmatize('purple'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a symptom of some physical hurt or disorder\n",
      "['the patient developed severe pain and distension']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    " \n",
    "syn = wordnet.synsets(\"pain\")\n",
    " \n",
    "print(syn[0].definition())\n",
    " \n",
    "print(syn[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://localhost:9000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [21/Mar/2018 18:01:01] \"POST /postagging HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Mar/2018 18:03:55] \"GET /postagging HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Mar/2018 18:03:59] \"POST /postagging HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from werkzeug.wrappers import Request, Response\n",
    "from flask import Flask, request, render_template, jsonify\n",
    "from textblob import TextBlob\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/count')\n",
    "def my_form():\n",
    "    return render_template('forms.html')\n",
    "\n",
    "@app.route('/count', methods=['POST'])\n",
    "def my_form_post():\n",
    "    if request.method == 'POST':\n",
    "        text = request.form['text']\n",
    "        processed_text = text.upper()\n",
    "        count_text = text.count(\"oi\")\n",
    "        return jsonify ({'count': count_text})\n",
    "\n",
    "@app.route('/tags')\n",
    "def my_form2():\n",
    "    return render_template('forms2.html')\n",
    "\n",
    "\n",
    "@app.route('/tags', methods=['POST'])\n",
    "def tags():\n",
    "    if request.method == 'POST':\n",
    "        text = request.form['text2']\n",
    "        blob = TextBlob(text)\n",
    "        tags = blob.tags\n",
    "        return jsonify ({'tags': tags})\n",
    "\n",
    "@app.route('/tokenization')\n",
    "def my_form3():\n",
    "    return render_template('forms3.html')\n",
    "\n",
    "\n",
    "@app.route('/tokenization', methods=['POST'])\n",
    "def tokens():\n",
    "    if request.method == 'POST':\n",
    "        text = request.form['text3']\n",
    "        doc = nlp(text)\n",
    "        tokenization = [token.orth_ for token in doc]\n",
    "        return jsonify({'tokenization': tokenization})\n",
    "\n",
    "\n",
    "@app.route('/lemmatisation')\n",
    "def my_form4():\n",
    "    return render_template('forms4.html')\n",
    "\n",
    "@app.route('/lemmatisation', methods=['POST'])\n",
    "def lemma():\n",
    "    if request.method == 'POST':\n",
    "        text = request.form['text4']\n",
    "        doc = nlp(text)\n",
    "        lemmatisation = [word.lemma_ for word in doc]\n",
    "        return jsonify({'lemmatisation': lemmatisation})\n",
    "\n",
    "@app.route('/postagging')\n",
    "def my_form5():\n",
    "    return render_template('forms5.html')\n",
    "\n",
    "@app.route('/postagging', methods=['POST'])\n",
    "def postag():\n",
    "    if request.method == 'POST':\n",
    "        text = request.form['text5']\n",
    "        #tagging = []\n",
    "        doc = nlp(text)\n",
    "        tokenization = [token.orth_ for token in doc]\n",
    "        postagging = [i.tag_ for i in doc]\n",
    "        return jsonify({'postagging': postagging})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from werkzeug.serving import run_simple\n",
    "    run_simple('localhost', 9000, app)\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://localhost:9000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [21/Mar/2018 17:54:02] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Mar/2018 17:54:02] \"GET /favicon.ico HTTP/1.1\" 404 -\n"
     ]
    }
   ],
   "source": [
    "from werkzeug.wrappers import Request, Response\n",
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello():\n",
    "    return \"Hello World!\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from werkzeug.serving import run_simple\n",
    "    run_simple('localhost', 9000, app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
